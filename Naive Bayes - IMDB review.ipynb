{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive bayes is a popular algorithm for classifying text. Although it is fairly simple, it often performs as well as much more complicated solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of being tired given that you ran: 0.6\n"
     ]
    }
   ],
   "source": [
    "# Here's a running history for the past week.\n",
    "# For each day, it contains whether or not the person ran, and whether or not they were tired.\n",
    "days = [[\"ran\", \"was tired\"], [\"ran\", \"was not tired\"], [\"didn't run\", \"was tired\"], [\"ran\", \"was tired\"], [\"didn't run\", \"was not tired\"], [\"ran\", \"was not tired\"], [\"ran\", \"was tired\"]]\n",
    "\n",
    "# Let's say we want to calculate the prob. that someone was tired given that they ran, using bayes' theorem.\n",
    "# This is P(A).\n",
    "prob_tired = len([d for d in days if d[1] == \"was tired\"])/(len(days))\n",
    "# This is P(B).\n",
    "prob_ran = len([d for d in days if d[0] == \"ran\"])/(len(days))\n",
    "# This is P(B|A).\n",
    "prob_ran_given_tired = len([d for d in days if d[0] == \"ran\" and d[1] == \"was tired\"])/(len([d for d in days if d[1] == \"was tired\"]))\n",
    "\n",
    "# Now we can calculate P(A|B).\n",
    "prob_tired_given_ran = (prob_ran_given_tired * prob_tired)/prob_ran\n",
    "\n",
    "print(\"Probability of being tired given that you ran: {0}\".format(prob_tired_given_ran))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We are working with one long string instead of several features. The easiest way to generate features from text is to split the text up into words. Each word in a review will then be a feature that we can then work with. In order to do this, we'll split the reviews based on whitespace.\n",
    "1. We'll then count up how many times each word occurs in the negative reviews;\n",
    "2. How many times each word occurs in the positive reviews. \n",
    "3. This will allow us to eventually compute the probabilities of a new review belonging to each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative text sample: this 22 minute short, short of a precursor to the later much better \"rock and rule\", features two fo\n",
      "Positive text sample: ladies and gentlemen: the show begins with this documentary film. it's structured in three chapters,\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Read in the training data.\n",
    "with open(\"movie_data.csv\", 'r') as file:\n",
    "    reviews = list(csv.reader(file))\n",
    "\n",
    "def get_text(reviews, score):\n",
    "    # Join together the text in the reviews for a particular tone.\n",
    "    #We lowercase to avoid \"Not\" and \"not\" being seen as different words, for example.\n",
    "    return \" \".join([r[0].lower() for r in reviews if r[1] == str(score)])\n",
    "\n",
    "def count_text(text):\n",
    "    # Split text into words based on whitespace.  Simple but effective.\n",
    "    words = re.split(\"\\s+\", text)\n",
    "    # Count up the occurence of each word.\n",
    "    return Counter(words) #Dict subclass for counting hashable items.  Sometimes called a bag or multiset.  \n",
    "                        #Elements are stored as dictionary keys and their counts are stored as dictionary values.\n",
    "\n",
    "negative_text = get_text(reviews, 0)\n",
    "positive_text = get_text(reviews, 1)\n",
    "# Generate word counts for negative tone.\n",
    "negative_counts = count_text(negative_text) #hash map key: word and value: word count.\n",
    "# Generate word counts for positive tone.\n",
    "positive_counts = count_text(positive_text)\n",
    "\n",
    "print(\"Negative text sample: {0}\".format(negative_text[:100]))\n",
    "print(\"Positive text sample: {0}\".format(positive_text[:100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considered,\n",
      "considered.\n",
      "\"unsubtle\"\n",
      "while..<br\n",
      "8mm.\n",
      "8mm,\n",
      "considered\"\n",
      "shakespearean-child\n",
      "considered?\n",
      "woods\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "count= 0\n",
    "for i in positive_counts:\n",
    "    print(i)\n",
    "    count =count+1\n",
    "    if count == 10:\n",
    "        break\n",
    "print(positive_counts.get('considered.', 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "considered,\n",
      "considered.\n",
      "'daring',\n",
      "emerges,\n",
      "thrice-cursed\n",
      "nunnery\n",
      "canada\"\n",
      "8mm.\n",
      "8mm,\n",
      "sunflowers!\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "count= 0\n",
    "for i in negative_counts:\n",
    "    print(i)\n",
    "    count =count+1\n",
    "    if count == 10:\n",
    "        break\n",
    "print(negative_counts.get('considered.', 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have the word counts, we just have to convert them to probabilities and multiply them out to get the predicted classification. \n",
    "1. Let's say we wanted to find the probability that the review **didn't like it** expresses a negative sentiment. \n",
    "2. We would find the total number of times the word **didn't occured in the negative reviews**, and **divide it by the total number of words in the negative reviews to get the probability of x given y.** \n",
    "3. We would then **do the same** for **like** and **it**. We would multiply all three probabilities, and then multiply by the **probability of any document expressing a negative sentiment** to get our final probability that the sentence expresses negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def get_y_count(score):\n",
    "    # Compute the count of each classification occuring in the data.\n",
    "    return len([r for r in reviews if r[1] == str(score)])\n",
    "\n",
    "# We need these counts to use for smoothing when computing the prediction.\n",
    "positive_review_count = get_y_count(1)\n",
    "negative_review_count = get_y_count(0)\n",
    "\n",
    "# These are the class probabilities (we saw them in the formula as P(y)).\n",
    "prob_positive = positive_review_count / len(reviews)\n",
    "prob_negative = negative_review_count / len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4999900002\n",
      "0.4999900002\n"
     ]
    }
   ],
   "source": [
    "print(prob_positive)\n",
    "print(prob_negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['review', 'sentiment']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This 22 minute short, short of a precursor to the later much better \"Rock and Rule\", features two folk singer mice who are going nowhere. The female mouse, Jan, signs a deal with the devil to become a hit rock star. So it's up to Daniel Mouse to save her soul. Made in the late '70's this has all the trappings of said decade (crap music, crap clothing and hair style, awful folk tunes) This cartoon is featured on the Second disc of the 2-Disk Collector's Edition of \"Rock and Rule\", it also comes with a Making of that runs almost as long as the show itself.<br /><br />My Grade: D+\n",
      "Negative prediction: 0.0\n",
      "Positive prediction: 0.0\n"
     ]
    }
   ],
   "source": [
    "def make_class_prediction(text, counts, class_prob, class_count):\n",
    "    prediction = 1\n",
    "    text_counts = Counter(re.split(\"\\s+\", text))\n",
    "    for word in text_counts:\n",
    "        # For every word in the text, we get the number of times that word occured in the reviews for a given class, add 1 to smooth the value, and divide by the total number of words in the (negative|positive) reviews (plus the class_count to also smooth the denominator).\n",
    "        # Smoothing ensures that we don't multiply the prediction by 0 if the word didn't exist in the training data.\n",
    "        # We also smooth the denominator counts to keep things even.\n",
    "        \n",
    "        #print(counts.get(word, 0)) #some words occurs often which diminishes the prediction value\n",
    "        \n",
    "        #text_counts: counts the occurence of each word (if a word occurs twice, its probability should be count twice)\n",
    "        \n",
    "        prediction *=  text_counts.get(word) * ((counts.get(word, 0) + 1) / (sum(counts.values()) + class_count))\n",
    "        # Now we multiply by the probability of the class existing in the documents.\n",
    "    return (prediction * class_prob)\n",
    "\n",
    "# As you can see, we can now generate probabilities for which class a given review is part of.\n",
    "# The probabilities themselves aren't very useful -- we make our classification decision based on which value is greater.\n",
    "print(\"Review: {0}\".format(reviews[1][0]))\n",
    "print(\"Negative prediction: {0}\".format(make_class_prediction(reviews[1][0], negative_counts, prob_negative, negative_review_count)))\n",
    "print(\"Positive prediction: {0}\".format(make_class_prediction(reviews[1][0], positive_counts, prob_positive, positive_review_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative prediction: 2.10446027557e-95\n",
      "Positive prediction: 4.00031344778e-95\n"
     ]
    }
   ],
   "source": [
    "text = 'This 22 minute short, short of a precursor to the later much better \"Rock and Rule\", features two folk singer mice who are going nowhere'\n",
    "print(\"Negative prediction: {0}\".format(make_class_prediction(text, negative_counts, prob_negative, negative_review_count)))\n",
    "print(\"Positive prediction: {0}\".format(make_class_prediction(text, positive_counts, prob_positive, positive_review_count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are a lot of extensions that we could make to this algorithm to make it perform better. \n",
    "1. We could look at n-grams instead of unigrams. \n",
    "2. We could remove punctuation and other non-characters. \n",
    "3. We could remove stopwords. \n",
    "4. We could also perform stemming or lemmatization.\n",
    "\n",
    "### Let's remove the stopwords. An easier way to use naive bayes is to use the implementation in scikit-learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "\n",
    "# Generate counts from text using a vectorizer.  There are other vectorizers available, and lots of options you can set.\n",
    "# This performs our step of computing word counts.\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "train_features = vectorizer.fit_transform([r[0] for r in reviews[1:20000]])\n",
    "test_features = vectorizer.transform([r[0] for r in reviews[20000:]])\n",
    "\n",
    "# Fit a naive bayes model to the training data.\n",
    "# This will train the model using the word counts we compute, and the existing classifications in the training set.\n",
    "nb = MultinomialNB()\n",
    "nb.fit(train_features, [int(r[1]) for r in reviews[1:20000]])\n",
    "\n",
    "# Now we can use the model to predict classifications for our test features.\n",
    "predictions = nb.predict(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30001"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30001"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews[20000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of correct labeled points out of a total 30001 points : 25503\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of correct labeled points out of a total %d points : %d\"\n",
    "...       % (len(reviews[20000:]),([int(r[1]) for r in reviews[20000:]] == predictions).sum()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
